{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions implements the forward propagation of a deep NN\n",
    "\n",
    "3 seperate functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function # One\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    This function computes the linear part of the forward propagation module and returns Z\n",
    "    \n",
    "    Arguments:\n",
    "        A: activation from the previous layer or  input data --- shape (size of previous layer, # of examples)\n",
    "        W: weights matrix --- dimension (layer_dims[l], layer_dims[l-1])\n",
    "        b: biases array   --- dimension (layer_dims[l], 1)\n",
    "        \n",
    "    returns:\n",
    "        Z: the input to the activation function --- also called (pre-activation parameter)\n",
    "    cache: Python dictionary storing 'A', 'W', 'b' to compute backward pass effeciently\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepNN_LinActivation_forward(A_previous, W, b, activation):\n",
    "    \"\"\"\n",
    "    This function implements the activation step for a linear output Z\n",
    "    \n",
    "    Arguments:\n",
    "        A_previous: activations from previous layer (or input data): (size of previous layer, number of examples\n",
    "                 W: Weights matrix, array of shape (layer_dims(l), layer_dims(l - 1)) \n",
    "                 b: bias vector, \n",
    "        activation: stored as a string, either \"sigmoid\" or \"ReLu\"\n",
    "\n",
    "    returns:\n",
    "        A: the output of the activation function, also called the post-activation value \n",
    "    cache: a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_previous, W, b)\n",
    "        A, activation_cache = sigmoid(Z) # a function called sigmoid which computes the sigmoid transformation on Z\n",
    "    \n",
    "    elif activation ==\"ReLu\":\n",
    "        Z, linear_cache = linear_forward(A_previous, W, b)\n",
    "        A, activation_cache = ReLu(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepNN_forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    This function  implements the forward propagation of a deep NN\n",
    "    \n",
    "    Arguments:\n",
    "        X: array with the training examples of the shape (input size, No. of examples)\n",
    "        parameters: Python dictionary with the weights and biases for each deep layer --- output of the function\n",
    "                    DeepNN_initialize\n",
    "    \n",
    "    returns:\n",
    "        AL: las post activation value\n",
    "        cache: \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

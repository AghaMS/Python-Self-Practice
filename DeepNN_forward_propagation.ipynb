{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions implements the forward propagation of a deep NN\n",
    "\n",
    "3 seperate functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed packages:\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    This function implements the sigmoid transformation on the value Z\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        Z: Any scalar or numpy array of any size - in this case Z is the pre-activation value\n",
    "        \n",
    "    returns:\n",
    "        Sigmoid: Transformed value of Z        \n",
    "    \"\"\"\n",
    "    \n",
    "    Sigmoid = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    return Sigmoid, Z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(Z):\n",
    "    \"\"\"\n",
    "    This function implements the rectified linear unit transformation on the value Z\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        Z: Any scalar or numpy array of any size - in this case Z is the pre-activation value\n",
    "        \n",
    "    returns:\n",
    "        ReLu: Transformed value of Z   \n",
    "    \"\"\"\n",
    "    \n",
    "    relu = np.maximum(0,Z)\n",
    "    assert (relu.shape == Z.shape)\n",
    "    \n",
    "    return relu, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function # One\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    This function computes the linear part of the forward propagation module and returns Z\n",
    "    \n",
    "    Arguments:\n",
    "        A: activation from the previous layer or  input data --- shape (size of previous layer, # of examples)\n",
    "        W: weights matrix --- dimension (layer_dims[l], layer_dims[l-1])\n",
    "        b: biases array   --- dimension (layer_dims[l], 1)\n",
    "        \n",
    "    returns:\n",
    "        Z: the input to the activation function --- also called (pre-activation parameter)\n",
    "    cache: Python dictionary storing 'A', 'W', 'b' to compute backward pass effeciently\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepNN_LinActivation_forward(A_previous, W, b, activation):\n",
    "    \"\"\"\n",
    "    This function implements the activation step for a linear output Z\n",
    "    \n",
    "    Arguments:\n",
    "        A_previous: activations from previous layer (or input data): (size of previous layer, number of examples\n",
    "                 W: Weights matrix, array of shape (layer_dims(l), layer_dims(l - 1)) \n",
    "                 b: bias vector, \n",
    "        activation: stored as a string, either \"sigmoid\" or \"ReLu\"\n",
    "\n",
    "    returns:\n",
    "        A: the output of the activation function, also called the post-activation value \n",
    "    cache: a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_previous, W, b)\n",
    "        A, activation_cache = sigmoid(Z) \n",
    "    \n",
    "    elif activation ==\"ReLu\":\n",
    "        Z, linear_cache = linear_forward(A_previous, W, b)\n",
    "        A, activation_cache = ReLu(Z) \n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepNN_forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    This function  implements the forward propagation of a deep NN\n",
    "    \n",
    "    Arguments:\n",
    "        X: array with the training examples of the shape (input size, No. of examples)\n",
    "        parameters: Python dictionary with the weights and biases for each deep layer --- output of the function\n",
    "                    DeepNN_initialize\n",
    "    \n",
    "    returns:\n",
    "        AL: last post activation value\n",
    "        caches:  list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)            \n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    \n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A, cache = DeepNN_LinActivation_forward(A_prev, parameters['W{:d}'.format(l)], parameters['b{:d}'.format(l)], activation = 'ReLu')\n",
    "        caches.append(cache)\n",
    "       \n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = DeepNN_LinActivation_forward(A_prev, parameters['W{:d}'.format(l)], parameters['b{:d}'.format(l)], activation = 'sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    \"\"\"\n",
    "    This function calculates the cost of a L_layers deep NN\n",
    "    \n",
    "    arguments:\n",
    "        AL: last post activation value, probability vector corresponding to the predicted labels\n",
    "        Y: actual labels of the training examples\n",
    "    \n",
    "    returns:\n",
    "        cost J: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = (-1 / m) * np.sum(Y*np.log(AL) + (1 - Y)*np.log(1 - AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
